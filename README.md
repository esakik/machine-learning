# machine-learning-nutsnbolts

> The nuts and bolts of machine learning and insights into its operations. Generated by DALL·E 3.

![DALL·E 2024-02-08 18 09 28 - Create a wide-format, detailed illustration that visualizes _The nuts and bolts of machine learning_ without including any characters or letters  This](https://github.com/esakik/machine-learning-nutsnbolts/assets/44774033/845c0482-d8e6-41c1-8d10-827289af28c8)

This repository aims to organize implementations of machine learning algorithms and demonstrate practical ways to apply or use machine learning models.
I initiated this project to refresh and reinforce my fading knowledge of previously learned concepts, deepen my understanding of various algorithms, and examine examples of their practical applications.

## Algorithms

Here is an overview of machine learning algorithms that are either currently implemented or planned for future implementation.
Additionally, this table presents supplementary materials I have prepared to revisit and strengthen my understanding of what I learned.

Notes: The "Colab" links will take you to the Google Colab notebook, where you can view the implementation and run the code. The "DEV" column contains links to articles I have written.

| Category      | Algorithms                                                                                                                                                  | Colab                                                                                                                                                                                                                                                                                                     | DEV                                                                                                                                                                                                                    |
|:--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Optimization  | Exploring Gradient Descent Variants: Batch, Mini-Batch, Stochastic, and Fundamentals of Implementation                                                      | [![Gradient descent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/optimization/gradient_descent.ipynb)                                                                                     | [![Dev.to blog](https://img.shields.io/badge/dev.to-0A0A0A?style=flat&logo=dev.to&logoColor=white)](https://dev.to/esakik/exploring-gradient-descent-after-implementing-linear-regression-from-scratch-i4e)            |
| Optimization  | Comparing Optimizers: Gradient Descent, Momentum, AdaGrad, and Adam Implemented from Scratch                                                                | [![Optimization algorithms](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/optimization/optimizers_comparison.ipynb)                                                                         |                                                                                                                                                                                                                        |
| Preprocessing | Implementing Feature Scaling Methods: Standardization (Z-score normalization) and Normalization (Min-Max scaling)                                           | [![Feature scaling (Standardization / Normalization)](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/preprocessing/feature_scaling.ipynb)                                                    | [![Dev.to blog](https://img.shields.io/badge/dev.to-0A0A0A?style=flat&logo=dev.to&logoColor=white)](https://dev.to/esakik/deciphering-standardization-and-normalization-understanding-feature-scaling-techniques-1cf5) |
| Supervised    | Implementing Linear Regression from Scratch: Gradient Descent, Mean Squared Error (MSE) Loss, and L1/L2 Regularization Explained                            | [![Linear regression / Regularization](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/supervised/linear_regression.ipynb)                                                                    | [![Dev.to blog](https://img.shields.io/badge/dev.to-0A0A0A?style=flat&logo=dev.to&logoColor=white)](https://dev.to/esakik/re-learn-linear-regression-in-python-from-theory-to-practice-277m)                           |
| Supervised    | Implementing Logistic Regression for Classification from Scratch: Cross-entropy loss, One-hot encoding, and Activation function (Sigmoid/Softmax) Explained | [![Logistic regression for classification / Activation function (Sigmoid / Softmax) / One-hot encoding](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/supervised/logistic_regression.ipynb) | [![Dev.to blog](https://img.shields.io/badge/dev.to-0A0A0A?style=flat&logo=dev.to&logoColor=white)](https://dev.to/esakik/brushing-up-on-logistic-regression-in-python-theory-to-practice-5ef4)                        |
| Supervised    | Implementing Neural Network for Classification with ReLU and Backpropagation from Scratch to PyTorch                                                        | [![Neural Network for classification](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/supervised/neural_network_classification.ipynb)                                                         |                                                                                                                                                                                                                        |
| Supervised    | Implementing K-Nearest Neighbor (k-NN) for Classification from Scratch                                                                                      | [![k-NN for classification](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/supervised/knn_classification.ipynb)                                                                              | [![Dev.to blog](https://img.shields.io/badge/dev.to-0A0A0A?style=flat&logo=dev.to&logoColor=white)](https://dev.to/esakik/brushing-up-on-k-nn-for-classification-in-python-theory-to-practice-phm)                     |
| Unsupervised  | Implementing K-means for Clustering from Scratch                                                                                                            | [![k-means for clustering](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/unsupervised/kmeans_clustering.ipynb)                                                                              |                                                                                                                                                                                                                        |
| Unsupervised  | Implementing Principal Component Analysis (PCA) for Dimensionality Reduction from Scratch                                                                   | [![PCA for dimensionality reduction](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/unsupervised/pca_dimensionality_reduction.ipynb)                                                         |                                                                                                                                                                                                                        |
| Reinforcement | Implementing Multi-Armed Bandit (MAB) from Scratch                                                                                                          | [![Multi-Armed Bandit (MAB) and Reinforcement learning](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/esakik/machine-learning-nutsnbolts/blob/main/algorithms/reinforcement/multi_armed_bandits_ε_greedy.ipynb)                                     |                                                                                                                                                                                                                        |
                                                                                                                                                                                                             
The above implementations are based on the following resources:

- https://www.coursera.org/learn/machine-learning
- https://github.com/oreilly-japan/deep-learning-from-scratch
- https://www.oreilly.com/library/view/introduction-to-machine/9781449369880
