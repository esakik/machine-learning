{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56b7db-be6a-4e2a-8a13-51dce4b47638",
   "metadata": {},
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764dffc-1a47-4fbd-bf9c-f3a03a77419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40b36d-a3d3-44fb-9a40-a934dc4b93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U numpy==1.23.5 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.4 seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817ba6c-cd95-4064-95cc-34f2918d5302",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0177f-360e-4a7e-b830-a87a36f854ce",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251eede-cfbc-4304-a1df-55e306b3c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=0)\n",
    "\n",
    "display(pd.concat([X, y], axis=1).head())\n",
    "print(\"samples: {}; features: {}\".format(*X.shape))\n",
    "print(\"samples: {}; values: {}\".format(*y.shape, y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc3ebd-fbdc-4325-a8ae-509b42f65a4d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04257eb5-7f28-477e-bc44-30ca2a0b3621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def standardize(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize the dataset. (z-score normalization)\n",
    "    :param X: The dataset to be standardized.\n",
    "    :return: The standardized dataset.\n",
    "    \"\"\"\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "\n",
    "X_train_std = standardize(X_train)\n",
    "X_test_std = standardize(X_test)\n",
    "display(X_train_std.head(3))\n",
    "display(X_test_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2bc33-b41c-42b6-bfb2-5004851839c8",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d30b2b-5e74-41e1-b877-65c4073791f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha: float = 0.01, eps: float = 1e-6) -> None:\n",
    "        self.alpha = alpha  # Learning rate for gradient descent\n",
    "        self.eps = eps  # Threshold of convergence\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"BinaryLogisticRegression\":\n",
    "        \"\"\"Fit the model to the training dataset. Optimizing the parameters by gradient descent.\n",
    "\n",
    "        :param X: The training dataset.\n",
    "        :param y: The target.\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "        self._m = X.shape[0]  # The number of samples\n",
    "        num_features = X.shape[1]  # The number of features\n",
    "\n",
    "        self._theta = np.zeros(num_features)  # The parameters (weight)\n",
    "\n",
    "        self._error_values = []  # The output values of the cost function in each iteration\n",
    "        self._grad_values = []  # Gradient values in each iteration\n",
    "        self._iter_counter = 0  # The counter of iterations\n",
    "        \n",
    "        error = self.J(X, y)  # The initial output value of the cost function with random parameters\n",
    "        diff = 1.0  # The difference between the previous and the current output values of the cost function\n",
    "\n",
    "        # Repeat until convergence\n",
    "        while diff > self.eps:\n",
    "            # Update the parameters by gradient descent\n",
    "            grad = (1 / self._m) * np.dot(self.h(X, self._theta) - y, X)  # Calculate the gradient using the formula\n",
    "            self._theta = self._theta - self.alpha * grad  # Update the parameters\n",
    "\n",
    "            # Print the current status\n",
    "            _error = self.J(X, y)  # Compute the error with the updated parameters\n",
    "            diff = abs(error - _error)  # Compute the difference between the previous and the current error\n",
    "            error = _error  # Update the error\n",
    "            self._error_values.append(error)\n",
    "            self._grad_values.append(grad.sum())\n",
    "            self._iter_counter += 1\n",
    "            print(f\"[{self._iter_counter}] error: {error}, diff: {diff}, grad: {grad.sum()}\")\n",
    "        print(f\"Convergence in {self._iter_counter} iterations.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predict the target values.\n",
    "\n",
    "        :param X: The dataset to be predicted.\n",
    "        :return: The predicted target values.\n",
    "        \"\"\"\n",
    "        return np.where(self.h(X, self._theta) >= 0.5, 1, 0)\n",
    "\n",
    "    def activate(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Activation function (sigmoid/logistic function).\n",
    "\n",
    "        :param z: The output of the hypothesis function.\n",
    "        :return: The activated output. 0 <= activate(z) <= 1\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def h(self, X: pd.DataFrame, theta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Hypothesis function.\n",
    "\n",
    "        :param X: The dataset\n",
    "        :param theta: The parameters (weight)\n",
    "        :return: The activated output. 0 <= h(x, theta) <= 1\n",
    "        \"\"\"\n",
    "        return self.activate(np.dot(X, theta))\n",
    "\n",
    "    def J(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Cost function (cross-entropy loss).\n",
    "\n",
    "        :param X: The dataset\n",
    "        :param y: The target\n",
    "        :return: The loss value.\n",
    "        \"\"\"\n",
    "        delta = 1e-7  # To avoid log(0)\n",
    "        return - (1 / self._m) * (\n",
    "            np.sum(y * np.log(self.h(X, self._theta) + delta) + (1 - y) * np.log(1 - self.h(X, self._theta) + delta))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbc117-ca45-489f-a5f9-cffe9cd6cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression()\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ca29e-00be-4370-aabb-a20456b95dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Cross-entropy Loss\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._error_values, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Gradient\")\n",
    "ax.set_ylabel(\"Gradient\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._grad_values, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a71647-4d16-403b-8783-f2c22aa2637c",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89d074-9e12-44ff-a088-bf0ea08662f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_pred = model.predict(X_train_std)\n",
    "print(f\"Acuracy score for train data: {accuracy_score(y_train, y_train_pred)}\")\n",
    "y_test_pred = model.predict(X_test_std)\n",
    "print(f\"Acuracy score for test data: {accuracy_score(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655341f7-b7df-4045-b9d1-67b77d38b4be",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5c631-ebd1-4864-a676-0ed1f2fa25b3",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434792f-60b0-44fc-9aa6-3b18d7252f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=0)\n",
    "\n",
    "display(pd.concat([X, y], axis=1).head())\n",
    "print(\"samples: {}; features: {}\".format(*X.shape))\n",
    "print(\"samples: {}; values: {}\".format(*y.shape, y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a83ef-5fd8-47b5-be26-29d18a49055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(pd.concat([X, y], axis=1), hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844a3e5-7ae2-4e9e-8c1c-0ef2c63da2da",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713507f-b3f2-489b-9bc7-b383d6f90685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding\n",
    "y_train_encoded = pd.get_dummies(y_train, dtype=int)\n",
    "print(y_train.head(3))\n",
    "print(\"\\n⬇️\\n\")\n",
    "y_train_encoded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125bac3e-3140-4628-9c1f-77ff4d39ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize the dataset. (z-score normalization)\n",
    "    :param X: The dataset to be standardized.\n",
    "    :return: The standardized dataset.\n",
    "    \"\"\"\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "\n",
    "X_train_std = standardize(X_train)\n",
    "X_test_std = standardize(X_test)\n",
    "display(X_train_std.head(3))\n",
    "display(X_test_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72097e99-fa8b-43d4-afa6-dfbfdc894849",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132ff1f-b51c-447f-8973-c59bd24ee891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha: float = 0.01, eps: float = 1e-6) -> None:\n",
    "        self.alpha = alpha  # Learning rate for gradient descent\n",
    "        self.eps = eps  # Threshold of convergence\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"MultipleLogisticRegression\":\n",
    "        \"\"\"Fit the model to the training dataset. Optimizing the parameters by gradient descent.\n",
    "\n",
    "        :param X: The training dataset.\n",
    "        :param y: The target.\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "        self._m = X.shape[0]  # The number of samples\n",
    "        num_features = X.shape[1]  # The number of features\n",
    "        num_targets = y.shape[1]  # The number of targets\n",
    "\n",
    "        self._theta = np.zeros([num_targets, num_features])  # The parameters (weight)\n",
    "\n",
    "        self._error_values = []  # The output values of the cost function in each iteration\n",
    "        self._grad_values = []  # Gradient values in each iteration\n",
    "        self._iter_counter = 0  # The counter of iterations\n",
    "        \n",
    "        error = self.J(X, y)  # The initial output value of the cost function with random parameters\n",
    "        diff = np.ones(num_targets)  # The difference between the previous and the current output values of the cost function\n",
    "\n",
    "        # Repeat until convergence\n",
    "        while diff.sum() > self.eps:\n",
    "            # Update the parameters by gradient descent\n",
    "            grad = (1 / self._m) * np.dot((self.h(X, self._theta) - y).T, X)  # Calculate the gradient using the formula\n",
    "            self._theta = self._theta - self.alpha * grad  # Update the parameters\n",
    "\n",
    "            # Print the current status\n",
    "            _error = self.J(X, y)  # Compute the error with the updated parameters\n",
    "            diff = abs(error - _error)  # Compute the difference between the previous and the current error\n",
    "            error = _error  # Update the error\n",
    "            self._error_values.append(error.sum())\n",
    "            self._grad_values.append(grad.sum())\n",
    "            self._iter_counter += 1\n",
    "            print(f\"[{self._iter_counter}] error: {error.sum()}, diff: {diff.sum()}, grad: {grad.sum()}\")\n",
    "        print(f\"Convergence in {self._iter_counter} iterations.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predict the target values.\n",
    "\n",
    "        :param X: The dataset to be predicted.\n",
    "        :return: The predicted target values.\n",
    "        \"\"\"\n",
    "        return self.h(X, self._theta).argmax(1)\n",
    "\n",
    "    def activate(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Activation function (sigmoid/logistic function).\n",
    "\n",
    "        :param z: The output of the hypothesis function.\n",
    "        :return: The activated output. 0 <= activate(z) <= 1\n",
    "        \"\"\"\n",
    "        return np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def h(self, X: pd.DataFrame, theta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Hypothesis function.\n",
    "\n",
    "        :param X: The dataset\n",
    "        :param theta: The parameters (weight)\n",
    "        :return: The activated output. 0 <= h(x, theta) <= 1\n",
    "        \"\"\"\n",
    "        return self.activate(np.dot(X, theta.T))\n",
    "\n",
    "    def J(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Cost function (cross-entropy loss).\n",
    "\n",
    "        :param X: The dataset\n",
    "        :param y: The target\n",
    "        :return: The loss value.\n",
    "        \"\"\"\n",
    "        delta = 1e-7  # To avoid log(0)\n",
    "        return - (1 / self._m) * (\n",
    "            np.sum(y * np.log(self.h(X, self._theta) + delta) + (1 - y) * np.log(1 - self.h(X, self._theta) + delta))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22a6dc-c485-4281-bfca-ff16e596f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultipleLogisticRegression()\n",
    "model.fit(X_train_std, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525d00b-a573-410e-81ea-ca31bdc758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Cross-entropy Loss\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._error_values, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Gradient\")\n",
    "ax.set_ylabel(\"Gradient\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._grad_values, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08bc6c-f455-4c3d-95c2-5c3e4ce02aff",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d3db9-5b83-42b4-83df-392330a952c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_pred = model.predict(X_train_std)\n",
    "print(f\"Accuracy score for train data: {accuracy_score(y_train, y_train_pred)}\")\n",
    "y_test_pred = model.predict(X_test_std)\n",
    "print(f\"Accuracy score for test data: {accuracy_score(y_test, y_test_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
