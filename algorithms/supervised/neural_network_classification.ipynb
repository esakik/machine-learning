{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7827c28e-5c86-435d-af52-76cb40f92a28",
   "metadata": {},
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231beaa0-e863-4560-a581-174debe130a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:17.392760Z",
     "start_time": "2024-02-10T12:55:17.388715Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8fb3f-1b20-40fc-818f-1f996b7262c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:19.827079Z",
     "start_time": "2024-02-10T12:55:17.392911Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install numpy==1.23.5 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01083d00-1dec-4da4-8623-6ecbb504093e",
   "metadata": {},
   "source": [
    "# Downloading/Visualizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6380b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:24.986745Z",
     "start_time": "2024-02-10T12:55:19.831685Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dataset = datasets.fetch_openml(\"mnist_784\", version=1, parser=\"auto\")\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "plt.imshow(X.iloc[0].values.reshape(28, 28), cmap=\"gray\")  # 1x784 => 28x28 image of the first sample in the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63b7ad-efab-4ca4-8fb2-498808d23dbc",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9951cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:25.244160Z",
     "start_time": "2024-02-10T12:55:24.966846Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = np.array(X_train, dtype=\"float32\") / 255.0, np.array(X_test, dtype=\"float32\") / 255.0  # Normalizing pixel values\n",
    "y_train, y_test = np.array(pd.get_dummies(y_train), dtype=\"int32\"), np.array(pd.get_dummies(y_test), dtype=\"int32\")  # One-hot encoding target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8d459",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training Model Without Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947536c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:25.251395Z",
     "start_time": "2024-02-10T12:55:25.245036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Retrieved from https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "Image(url=\"https://github.com/esakik/machine-learning-nutsnbolts/assets/44774033/663a9570-12c7-49e4-92ad-2a15c76d9b2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369814c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:06:58.937954Z",
     "start_time": "2024-02-10T13:06:58.928835Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The sigmoid function.\n",
    "\n",
    "    :param x: The input data\n",
    "    :return: The sigmoid of the input data\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The softmax function.\n",
    "\n",
    "    :param x: The input data\n",
    "    :return: The softmax of the input data\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def cross_entropy_error(y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the cross-entropy error.\n",
    "\n",
    "    :param y_pred: The predicted data\n",
    "    :param y: The target data\n",
    "    :return: The cross-entropy error\n",
    "    \"\"\"\n",
    "    delta = 1e-7\n",
    "    return -np.sum(y * np.log(y_pred + delta))\n",
    "\n",
    "def accuracy(y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the accuracy.\n",
    "\n",
    "    :param y_pred: The predicted data\n",
    "    :param y: The target data\n",
    "    :return: The accuracy\n",
    "    \"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "\n",
    "class NeuralNetworkWithoutBackpropagation:\n",
    "    \"\"\"Simple Neural Network with 1 hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, weight_init_std: float = 0.01) -> None:\n",
    "        \"\"\"Initialize weights and biases.\n",
    "\n",
    "        :param input_size: The number of input neurons\n",
    "        :param hidden_size: The number of hidden neurons\n",
    "        :param output_size: The number of output neurons\n",
    "        :param weight_init_std: The standard deviation of the random weights\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "\n",
    "        self.W1 = self.weight_init_std * np.random.randn(self.input_size, self.hidden_size)  # The first layer's weights (input_size x hidden_size)\n",
    "        self.b1 = np.zeros(self.hidden_size)  # The first layer's biases (hidden_size)\n",
    "        self.W2 = self.weight_init_std * np.random.randn(self.hidden_size, self.output_size)  # The second layer's weights (hidden_size x output_size)\n",
    "        self.b2 = np.zeros(self.output_size)  # The second layer's biases (output_size)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1, batch_size: int = 100, learning_rate: float = 0.1) -> None:\n",
    "        \"\"\"Train the model. The model uses mini-batch Gradient Descent to update the weights and biases.\n",
    "        Since the model does not use backpropagation, the training process is not efficient.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :param epochs: The number of training iterations\n",
    "        :param batch_size: The number of samples to use in each training iteration\n",
    "        :param learning_rate: The learning rate\n",
    "        \"\"\"\n",
    "        print(f\"Training the model for {epochs} epochs with a batch size of {batch_size} and a learning rate of {learning_rate}\")\n",
    "        \n",
    "        iters_per_epoch = max(int(X.shape[0] / batch_size), 1)\n",
    "        print(f\"Number of iterations per epoch: {iters_per_epoch}\")\n",
    "\n",
    "        for i_epoch in range(1, epochs + 1):\n",
    "            for i_iter in range(1, iters_per_epoch + 1):\n",
    "                # Mini-batch\n",
    "                indices = np.random.choice(X.shape[0], batch_size)\n",
    "                X_batch, y_batch = X[indices], y[indices]\n",
    "    \n",
    "                # Forward pass (Prediction)\n",
    "                loss = self.loss(X_batch, y_batch)\n",
    "                self.train_loss_history.append(loss)\n",
    "                # print(f\"Epoch {i_epoch}/{epochs} - Iteration {i_iter}/{iters_per_epoch} - Loss: {loss}\")\n",
    "    \n",
    "                # Backward pass (Gradient Descent to update weights and biases)\n",
    "                self.W1 = self.W1 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W1)\n",
    "                self.b1 = self.b1 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b1)\n",
    "                self.W2 = self.W2 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W2)\n",
    "                self.b2 = self.b2 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b2)\n",
    "\n",
    "            # Train and test accuracy\n",
    "            train_acc = accuracy(self.predict(X), y)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Train accuracy: {train_acc}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the target data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: The predicted target data\n",
    "        \"\"\"\n",
    "        a1 = np.dot(X, self.W1) + self.b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, self.W2) + self.b2\n",
    "        z2 = softmax(a2)\n",
    "        return z2\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the loss.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :return: The loss\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return cross_entropy_error(y_pred, y)\n",
    "\n",
    "    def gradient_descent(self, f: callable, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param X: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            return self.gradient_descent_1d(f, X)\n",
    "        else:\n",
    "            grad = np.zeros_like(X)\n",
    "            for idx, x in enumerate(X):\n",
    "                grad[idx] = self.gradient_descent_1d(f, x)\n",
    "            return grad\n",
    "\n",
    "    def gradient_descent_1d(self, f: callable, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method. This method is only for 1D input data.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param x: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        h = 1e-4\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "\n",
    "            # f(x+h)\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x)\n",
    "\n",
    "            # f(x-h)\n",
    "            x[idx] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "\n",
    "            # Derivative\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "            x[idx] = tmp_val\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8cd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:07:00.096292Z",
     "start_time": "2024-02-10T13:07:00.091352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# model = NeuralNetworkWithoutBackpropagation(input_size=784, hidden_size=50, output_size=10)\n",
    "# model.fit(X_train, y_train, epochs=1000, batch_size=100, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8cb52",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232647a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:07:01.155723Z",
     "start_time": "2024-02-10T13:07:01.130765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"ReLU Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    \"\"\"Affine Layer.\"\"\"\n",
    "\n",
    "    def __init__(self, W: np.ndarray, b: np.ndarray) -> None:\n",
    "        \"\"\"Initialize the layer.\n",
    "\n",
    "        :param W: The weight\n",
    "        :param b: The bias\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    \"\"\"Softmax with Loss Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :param t: The target data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Simple Neural Network with 1 hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, weight_init_std: float = 0.01) -> None:\n",
    "        \"\"\"Initialize weights and biases.\n",
    "\n",
    "        :param input_size: The number of input neurons\n",
    "        :param hidden_size: The number of hidden neurons\n",
    "        :param output_size: The number of output neurons\n",
    "        :param weight_init_std: The standard deviation of the random weights\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "\n",
    "        self.params = {\n",
    "            \"W1\": self.weight_init_std * np.random.randn(self.input_size, self.hidden_size),  # The first layer's weights (input_size x hidden_size)\n",
    "            \"b1\": np.zeros(self.hidden_size),  # The first layer's biases (hidden_size),\n",
    "            \"W2\": self.weight_init_std * np.random.randn(self.hidden_size, self.output_size),  # The second layer's weights (hidden_size x output_size)\n",
    "            \"b2\": np.zeros(self.output_size),  # The second layer's biases (output_size)\n",
    "        }\n",
    "\n",
    "        self.layers = {\n",
    "            \"Affine1\": Affine(self.params[\"W1\"], self.params[\"b1\"]),\n",
    "            \"Relu1\": Relu(),\n",
    "            \"Affine2\": Affine(self.params[\"W2\"], self.params[\"b2\"]),\n",
    "        }\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 100, batch_size: int = 100, learning_rate: float = 0.0001) -> None:\n",
    "        \"\"\"Train the model.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :param epochs: The number of training iterations\n",
    "        :param batch_size: The number of samples to use in each training iteration\n",
    "        :param learning_rate: The learning rate\n",
    "        \"\"\"\n",
    "        print(f\"Training the model for {epochs} epochs with a batch size of {batch_size} and a learning rate of {learning_rate}\")\n",
    "\n",
    "        iters_per_epoch = max(int(X.shape[0] / batch_size), 1)\n",
    "        print(f\"Number of iterations per epoch: {iters_per_epoch}\")\n",
    "\n",
    "        for i_epoch in range(1, epochs + 1):\n",
    "            for i_iter in range(1, iters_per_epoch + 1):\n",
    "                # Mini-batch\n",
    "                indices = np.random.choice(X.shape[0], batch_size)\n",
    "                X_batch, y_batch = X[indices], y[indices]\n",
    "\n",
    "                # Forward pass (Prediction)\n",
    "                loss = self.loss(X_batch, y_batch)\n",
    "                self.train_loss_history.append(loss)\n",
    "                # print(f\"Epoch {i_epoch}/{epochs} - Iteration {i_iter}/{iters_per_epoch} - Loss: {loss}\")\n",
    "\n",
    "                # Backward pass (Gradient calculation using backpropagation)\n",
    "                grad = self.gradient()\n",
    "                for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "                    self.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "            # Train and test accuracy\n",
    "            train_acc = accuracy(self.predict(X), y)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Train accuracy: {train_acc}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the target data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: The predicted target data\n",
    "        \"\"\"\n",
    "        out = X.copy()\n",
    "        for layer in self.layers.values():\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the loss.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :return: The loss\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return self.last_layer.forward(y_pred, y)\n",
    "\n",
    "    def gradient(self) -> dict:\n",
    "        \"\"\"Calculate the gradients using backpropagation.\n",
    "\n",
    "        :return: The gradients\n",
    "        \"\"\"\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        for layer in reversed(self.layers.values()):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bd323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:59:50.462911Z",
     "start_time": "2024-02-10T12:59:30.560981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=100, learning_rate=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d1da8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Plot the Training Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7df20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:08:36.779674Z",
     "start_time": "2024-02-10T13:08:36.103831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.set_ylabel(\"Training Loss\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(model.train_loss_history, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"Training Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(model.train_acc_history, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
