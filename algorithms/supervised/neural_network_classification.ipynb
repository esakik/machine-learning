{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7827c28e-5c86-435d-af52-76cb40f92a28",
   "metadata": {},
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231beaa0-e863-4560-a581-174debe130a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:20:02.535908Z",
     "start_time": "2024-02-09T14:20:02.509520Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8fb3f-1b20-40fc-818f-1f996b7262c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:20:04.672493Z",
     "start_time": "2024-02-09T14:20:02.517712Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install -U numpy==1.23.5 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01083d00-1dec-4da4-8623-6ecbb504093e",
   "metadata": {},
   "source": [
    "# Downloading/Visualizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6380b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:16:22.108905Z",
     "start_time": "2024-02-09T14:16:16.910147Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dataset = datasets.fetch_openml(\"mnist_784\", version=1, parser=\"auto\")\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "plt.imshow(X.iloc[0].values.reshape(28, 28), cmap=\"gray\")  # 1x784 => 28x28 image of the first sample in the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63b7ad-efab-4ca4-8fb2-498808d23dbc",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9951cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:16:22.456645Z",
     "start_time": "2024-02-09T14:16:22.099503Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = np.array(X_train, dtype=\"float32\") / 255.0, np.array(X_test, dtype=\"float32\") / 255.0  # Normalizing pixel values\n",
    "y_train, y_test = np.array(pd.get_dummies(y_train), dtype=\"int32\"), np.array(pd.get_dummies(y_test), dtype=\"int32\")  # One-hot encoding target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8d459",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947536c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:16:22.465005Z",
     "start_time": "2024-02-09T14:16:22.460786Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Retrieved from https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "Image(url=\"https://github.com/esakik/machine-learning-nutsnbolts/assets/44774033/663a9570-12c7-49e4-92ad-2a15c76d9b2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369814c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:16:22.490187Z",
     "start_time": "2024-02-09T14:16:22.484691Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Simple Neural Network with 1 hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, weight_init_std: float = 0.01) -> None:\n",
    "        \"\"\"Initialize weights and biases.\n",
    "\n",
    "        :param input_size: The number of input neurons\n",
    "        :param hidden_size: The number of hidden neurons\n",
    "        :param output_size: The number of output neurons\n",
    "        :param weight_init_std: The standard deviation of the random weights\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "\n",
    "        self.W1 = self.weight_init_std * np.random.randn(self.input_size, self.hidden_size)  # The first layer's weights (input_size x hidden_size)\n",
    "        self.b1 = np.zeros(self.hidden_size)  # The first layer's biases (hidden_size)\n",
    "        self.W2 = self.weight_init_std * np.random.randn(self.hidden_size, self.output_size)  # The second layer's weights (hidden_size x output_size)\n",
    "        self.b2 = np.zeros(self.output_size)  # The second layer's biases (output_size)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.test_acc_history = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1, batch_size: int = 100, learning_rate: float = 0.1) -> None:\n",
    "        \"\"\"Train the model. The model uses mini-batch Gradient Descent to update the weights and biases.\n",
    "        Since the model does not use backpropagation, the training process is not efficient.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :param epochs: The number of training iterations\n",
    "        :param batch_size: The number of samples to use in each training iteration\n",
    "        :param learning_rate: The learning rate\n",
    "        \"\"\"\n",
    "        print(f\"Training the model for {epochs} epochs with a batch size of {batch_size} and a learning rate of {learning_rate}\")\n",
    "        \n",
    "        iters_per_epoch = max(int(X.shape[0] / batch_size), 1)\n",
    "        print(f\"Number of iterations per epoch: {iters_per_epoch}\")\n",
    "\n",
    "        for i_epoch in range(1, epochs + 1):\n",
    "            for i_iter in range(1, iters_per_epoch + 1):\n",
    "                # Mini-batch\n",
    "                indices = np.random.choice(X.shape[0], batch_size)\n",
    "                X_batch, y_batch = X[indices], y[indices]\n",
    "    \n",
    "                # Forward pass (Prediction)\n",
    "                loss = self.loss(X_batch, y_batch)\n",
    "                self.train_loss_history.append(loss)\n",
    "                print(f\"Epoch {i_epoch}/{epochs} - Iteration {i_iter}/{iters_per_epoch} - Loss: {loss}\")\n",
    "    \n",
    "                # Backward pass (Gradient Descent to update weights and biases)\n",
    "                self.W1 = self.W1 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W1)\n",
    "                self.b1 = self.b1 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b1)\n",
    "                self.W2 = self.W2 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W2)\n",
    "                self.b2 = self.b2 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b2)\n",
    "\n",
    "            # Train and test accuracy\n",
    "            train_acc = self.accuracy(X_train, y_train)\n",
    "            test_acc = self.accuracy(X_test, y_test)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            self.test_acc_history.append(test_acc)\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Train accuracy: {train_acc}\")\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Test accuracy: {test_acc}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the target data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: The predicted target data\n",
    "        \"\"\"\n",
    "        a1 = np.dot(X, self.W1) + self.b1\n",
    "        z1 = self.sigmoid(a1)\n",
    "        a2 = np.dot(z1, self.W2) + self.b2\n",
    "        z2 = self.softmax(a2)\n",
    "        return z2\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"The sigmoid function.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The sigmoid of the input data\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"The softmax function.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The softmax of the input data\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the loss.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :return: The loss\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return self.cross_entropy_error(y_pred, y)\n",
    "\n",
    "    def cross_entropy_error(self, y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the cross-entropy error.\n",
    "\n",
    "        :param y_pred: The predicted data\n",
    "        :param y: The target data\n",
    "        :return: The cross-entropy error\n",
    "        \"\"\"\n",
    "        delta = 1e-7\n",
    "        return -np.sum(y * np.log(y_pred + delta))\n",
    "\n",
    "    def accuracy(self, y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the accuracy.\n",
    "\n",
    "        :param y_pred: The predicted data\n",
    "        :param y: The target data\n",
    "        :return: The accuracy\n",
    "        \"\"\"\n",
    "        return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "    def gradient_descent(self, f: callable, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param X: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            return self.gradient_descent_1d(f, X)\n",
    "        else:\n",
    "            grad = np.zeros_like(X)\n",
    "            for idx, x in enumerate(X):\n",
    "                grad[idx] = self.gradient_descent_1d(f, x)\n",
    "            return grad\n",
    "\n",
    "    def gradient_descent_1d(self, f: callable, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method. This method is only for 1D input data.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param x: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        h = 1e-4\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "\n",
    "            # f(x+h)\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x)\n",
    "\n",
    "            # f(x-h)\n",
    "            x[idx] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "\n",
    "            # Derivative\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "            x[idx] = tmp_val\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8cd7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot the Training Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(model.train_loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-09T22:49:47.250094Z",
     "end_time": "2024-02-09T22:49:47.434742Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot the Training and Test Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"Training Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(model.train_acc_history, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(model.test_acc_history, color=\"r\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-09T22:54:07.503525Z",
     "end_time": "2024-02-09T22:54:07.786324Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
