{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7827c28e-5c86-435d-af52-76cb40f92a28",
   "metadata": {},
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231beaa0-e863-4560-a581-174debe130a2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-11T18:11:33.402750Z",
     "end_time": "2024-02-11T18:11:33.408939Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8fb3f-1b20-40fc-818f-1f996b7262c5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-11T18:12:25.278903Z",
     "end_time": "2024-02-11T18:12:27.091994Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install numpy==1.23.5 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.4 torch==2.2.0 torchvision==0.17.0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing Neural Network from Scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "01083d00-1dec-4da4-8623-6ecbb504093e",
   "metadata": {},
   "source": [
    "## Downloading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6380b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:24.986745Z",
     "start_time": "2024-02-10T12:55:19.831685Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dataset = datasets.fetch_openml(\"mnist_784\", version=1, parser=\"auto\")\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "plt.imshow(X.iloc[0].values.reshape(28, 28), cmap=\"gray\")  # 1x784 => 28x28 image of the first sample in the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63b7ad-efab-4ca4-8fb2-498808d23dbc",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9951cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:25.244160Z",
     "start_time": "2024-02-10T12:55:24.966846Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = np.array(X_train, dtype=\"float32\") / 255.0, np.array(X_test, dtype=\"float32\") / 255.0  # Normalizing pixel values\n",
    "y_train, y_test = np.array(pd.get_dummies(y_train), dtype=\"int32\"), np.array(pd.get_dummies(y_test), dtype=\"int32\")  # One-hot encoding target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8d459",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training Neural Network without Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947536c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:55:25.251395Z",
     "start_time": "2024-02-10T12:55:25.245036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Retrieved from https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "Image(url=\"https://github.com/esakik/machine-learning-nutsnbolts/assets/44774033/663a9570-12c7-49e4-92ad-2a15c76d9b2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369814c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:06:58.937954Z",
     "start_time": "2024-02-10T13:06:58.928835Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The sigmoid function.\n",
    "\n",
    "    :param x: The input data\n",
    "    :return: The sigmoid of the input data\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The softmax function.\n",
    "\n",
    "    :param x: The input data\n",
    "    :return: The softmax of the input data\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def cross_entropy_error(y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the cross-entropy error.\n",
    "\n",
    "    :param y_pred: The predicted data\n",
    "    :param y: The target data\n",
    "    :return: The cross-entropy error\n",
    "    \"\"\"\n",
    "    delta = 1e-7\n",
    "    return -np.sum(y * np.log(y_pred + delta))\n",
    "\n",
    "def accuracy(y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the accuracy.\n",
    "\n",
    "    :param y_pred: The predicted data\n",
    "    :param y: The target data\n",
    "    :return: The accuracy\n",
    "    \"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "\n",
    "class NeuralNetworkWithoutBackpropagation:\n",
    "    \"\"\"Simple Neural Network with 1 hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, weight_init_std: float = 0.01) -> None:\n",
    "        \"\"\"Initialize weights and biases.\n",
    "\n",
    "        :param input_size: The number of input neurons\n",
    "        :param hidden_size: The number of hidden neurons\n",
    "        :param output_size: The number of output neurons\n",
    "        :param weight_init_std: The standard deviation of the random weights\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "\n",
    "        self.W1 = self.weight_init_std * np.random.randn(self.input_size, self.hidden_size)  # The first layer's weights (input_size x hidden_size)\n",
    "        self.b1 = np.zeros(self.hidden_size)  # The first layer's biases (hidden_size)\n",
    "        self.W2 = self.weight_init_std * np.random.randn(self.hidden_size, self.output_size)  # The second layer's weights (hidden_size x output_size)\n",
    "        self.b2 = np.zeros(self.output_size)  # The second layer's biases (output_size)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1, batch_size: int = 100, learning_rate: float = 0.1) -> None:\n",
    "        \"\"\"Train the model. The model uses mini-batch Gradient Descent to update the weights and biases.\n",
    "        Since the model does not use backpropagation, the training process is not efficient.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :param epochs: The number of training iterations\n",
    "        :param batch_size: The number of samples to use in each training iteration\n",
    "        :param learning_rate: The learning rate\n",
    "        \"\"\"\n",
    "        print(f\"Training the model for {epochs} epochs with a batch size of {batch_size} and a learning rate of {learning_rate}\")\n",
    "\n",
    "        iters_per_epoch = max(int(X.shape[0] / batch_size), 1)\n",
    "        print(f\"Number of iterations per epoch: {iters_per_epoch}\")\n",
    "\n",
    "        for i_epoch in range(1, epochs + 1):\n",
    "            for i_iter in range(1, iters_per_epoch + 1):\n",
    "                # Mini-batch\n",
    "                indices = np.random.choice(X.shape[0], batch_size)\n",
    "                X_batch, y_batch = X[indices], y[indices]\n",
    "\n",
    "                # Forward pass (Prediction)\n",
    "                loss = self.loss(X_batch, y_batch)\n",
    "                self.train_loss_history.append(loss)\n",
    "                # print(f\"Epoch {i_epoch}/{epochs} - Iteration {i_iter}/{iters_per_epoch} - Loss: {loss}\")\n",
    "\n",
    "                # Backward pass (Gradient Descent to update weights and biases)\n",
    "                self.W1 = self.W1 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W1)\n",
    "                self.b1 = self.b1 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b1)\n",
    "                self.W2 = self.W2 - learning_rate * self.gradient_descent(lambda W: self.loss(X_batch, y_batch), self.W2)\n",
    "                self.b2 = self.b2 - learning_rate * self.gradient_descent(lambda b: self.loss(X_batch, y_batch), self.b2)\n",
    "\n",
    "            # Train and test accuracy\n",
    "            train_acc = accuracy(self.predict(X), y)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Train accuracy: {train_acc}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the target data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: The predicted target data\n",
    "        \"\"\"\n",
    "        a1 = np.dot(X, self.W1) + self.b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, self.W2) + self.b2\n",
    "        z2 = softmax(a2)\n",
    "        return z2\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the loss.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :return: The loss\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return cross_entropy_error(y_pred, y)\n",
    "\n",
    "    def gradient_descent(self, f: callable, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param X: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            return self.gradient_descent_1d(f, X)\n",
    "        else:\n",
    "            grad = np.zeros_like(X)\n",
    "            for idx, x in enumerate(X):\n",
    "                grad[idx] = self.gradient_descent_1d(f, x)\n",
    "            return grad\n",
    "\n",
    "    def gradient_descent_1d(self, f: callable, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the gradient using the finite difference method. This method is only for 1D input data.\n",
    "\n",
    "        :param f: The function to differentiate\n",
    "        :param x: The input data\n",
    "        :return: The gradient\n",
    "        \"\"\"\n",
    "        h = 1e-4\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "\n",
    "            # f(x+h)\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x)\n",
    "\n",
    "            # f(x-h)\n",
    "            x[idx] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "\n",
    "            # Derivative\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "            x[idx] = tmp_val\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8cd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:07:00.096292Z",
     "start_time": "2024-02-10T13:07:00.091352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The training process is very slow because the model does not use backpropagation\n",
    "## model = NeuralNetworkWithoutBackpropagation(input_size=784, hidden_size=50, output_size=10)\n",
    "## model.fit(X_train, y_train, epochs=1000, batch_size=100, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8cb52",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training Neural Network with Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232647a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:07:01.155723Z",
     "start_time": "2024-02-10T13:07:01.130765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"ReLU Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    \"\"\"Affine Layer.\"\"\"\n",
    "\n",
    "    def __init__(self, W: np.ndarray, b: np.ndarray) -> None:\n",
    "        \"\"\"Initialize the layer.\n",
    "\n",
    "        :param W: The weight\n",
    "        :param b: The bias\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    \"\"\"Softmax with Loss Layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        :param x: The input data\n",
    "        :param t: The target data\n",
    "        :return: The output data\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1) -> np.ndarray:\n",
    "        \"\"\"Backward propagation.\n",
    "\n",
    "        :param dout: The derivative of the output data\n",
    "        :return: The derivative of the input data\n",
    "        \"\"\"\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Simple Neural Network with 1 hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, weight_init_std: float = 0.01) -> None:\n",
    "        \"\"\"Initialize weights and biases.\n",
    "\n",
    "        :param input_size: The number of input neurons\n",
    "        :param hidden_size: The number of hidden neurons\n",
    "        :param output_size: The number of output neurons\n",
    "        :param weight_init_std: The standard deviation of the random weights\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "\n",
    "        self.params = {\n",
    "            \"W1\": self.weight_init_std * np.random.randn(self.input_size, self.hidden_size),  # The first layer's weights (input_size x hidden_size)\n",
    "            \"b1\": np.zeros(self.hidden_size),  # The first layer's biases (hidden_size),\n",
    "            \"W2\": self.weight_init_std * np.random.randn(self.hidden_size, self.output_size),  # The second layer's weights (hidden_size x output_size)\n",
    "            \"b2\": np.zeros(self.output_size),  # The second layer's biases (output_size)\n",
    "        }\n",
    "\n",
    "        self.layers = {\n",
    "            \"Affine1\": Affine(self.params[\"W1\"], self.params[\"b1\"]),\n",
    "            \"Relu1\": Relu(),\n",
    "            \"Affine2\": Affine(self.params[\"W2\"], self.params[\"b2\"]),\n",
    "        }\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 100, batch_size: int = 100, learning_rate: float = 0.0001) -> None:\n",
    "        \"\"\"Train the model.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :param epochs: The number of training iterations\n",
    "        :param batch_size: The number of samples to use in each training iteration\n",
    "        :param learning_rate: The learning rate\n",
    "        \"\"\"\n",
    "        print(f\"Training the model for {epochs} epochs with a batch size of {batch_size} and a learning rate of {learning_rate}\")\n",
    "\n",
    "        iters_per_epoch = max(int(X.shape[0] / batch_size), 1)\n",
    "        print(f\"Number of iterations per epoch: {iters_per_epoch}\")\n",
    "\n",
    "        for i_epoch in range(1, epochs + 1):\n",
    "            for i_iter in range(1, iters_per_epoch + 1):\n",
    "                # Mini-batch\n",
    "                indices = np.random.choice(X.shape[0], batch_size)\n",
    "                X_batch, y_batch = X[indices], y[indices]\n",
    "\n",
    "                # Forward pass (Prediction)\n",
    "                loss = self.loss(X_batch, y_batch)\n",
    "                self.train_loss_history.append(loss)\n",
    "                # print(f\"Epoch {i_epoch}/{epochs} - Iteration {i_iter}/{iters_per_epoch} - Loss: {loss}\")\n",
    "\n",
    "                # Backward pass (Gradient calculation using backpropagation)\n",
    "                grad = self.gradient()\n",
    "                for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "                    self.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "            # Train and test accuracy\n",
    "            train_acc = accuracy(self.predict(X), y)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            print(f\"Epoch {i_epoch}/{epochs} - Train accuracy: {train_acc}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the target data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: The predicted target data\n",
    "        \"\"\"\n",
    "        out = X.copy()\n",
    "        for layer in self.layers.values():\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the loss.\n",
    "\n",
    "        :param X: The input data\n",
    "        :param y: The target data\n",
    "        :return: The loss\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return self.last_layer.forward(y_pred, y)\n",
    "\n",
    "    def gradient(self) -> dict:\n",
    "        \"\"\"Calculate the gradients using backpropagation.\n",
    "\n",
    "        :return: The gradients\n",
    "        \"\"\"\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        for layer in reversed(self.layers.values()):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bd323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T12:59:50.462911Z",
     "start_time": "2024-02-10T12:59:30.560981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=100, learning_rate=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d1da8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Visualizing Training Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7df20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:08:36.779674Z",
     "start_time": "2024-02-10T13:08:36.103831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.set_ylabel(\"Training Loss\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(model.train_loss_history, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"Training Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(model.train_acc_history, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing Neural Network with PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T18:20:49.851435Z",
     "end_time": "2024-02-11T18:20:49.858307Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading FashionMNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "work_dir = %pwd\n",
    "data_dir = Path(work_dir).parents[1] / \"datasets\"\n",
    "\n",
    "train_data = datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=ToTensor())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:19:06.379754Z",
     "end_time": "2024-02-11T20:19:06.446900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in test_loader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)  # `batch_size` samples, 1 channel, 28x28\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)  # `batch_size` labels\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:19:21.639560Z",
     "end_time": "2024-02-11T20:19:21.671907Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork(input_size=28*28, hidden_size=512, output_size=10).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:19:23.688917Z",
     "end_time": "2024-02-11T20:19:23.699820Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizing Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:19:24.819884Z",
     "end_time": "2024-02-11T20:19:24.833622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> None:\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Move the data to the device that is used\n",
    "\n",
    "        # Compute prediction error (Forward pass)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backpropagation (Backward pass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset the gradients\n",
    "\n",
    "        # Display the progress\n",
    "        if i % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            progress = f\"{i * len(X):>5d}/{len(dataloader.dataset):>5d}\"\n",
    "            print(f\"loss: {loss:>7f}  [{progress}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:19:25.275235Z",
     "end_time": "2024-02-11T20:19:25.292850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    ") -> None:\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():  # Do not calculate the gradients\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= dataloader.batch_size\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:20:07.302151Z",
     "end_time": "2024-02-11T20:20:07.359740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:20:08.567257Z",
     "end_time": "2024-02-11T20:20:58.382704Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = Path(work_dir).parents[1] / \"models\" / \"nn_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved PyTorch Model State to {model_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:21:32.611821Z",
     "end_time": "2024-02-11T20:21:32.616418Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=28*28, hidden_size=512, output_size=10).to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:21:36.539894Z",
     "end_time": "2024-02-11T20:21:36.551845Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    predicted, actual = classes[y_pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T20:22:33.166561Z",
     "end_time": "2024-02-11T20:22:33.205998Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
