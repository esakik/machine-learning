{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d748bf7d-db94-40b0-b72e-9d13f4dfc0ef",
   "metadata": {},
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71da27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install -U numpy==1.23.5 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.4 seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7f8fd-65d6-4e42-8b4c-d915623f6c8c",
   "metadata": {},
   "source": [
    "# Downloading/Visualizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034a13c-b42a-4fd6-afe6-47784dc172af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dataset = datasets.fetch_california_housing()\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "y = pd.Series(data=dataset.target, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=0)\n",
    "\n",
    "display(X.head())\n",
    "print(\"samples: {}; features: {}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c3f3c-1ed5-45de-9fd3-7d22ca47c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y.head())\n",
    "print(\"samples: {}\".format(*y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9f878-26c2-4592-a071-e34c456ffe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = pd.concat([X, y], axis=1).corr()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(\"Correlation Matrix\")\n",
    "sns.heatmap(corr_matrix, annot=True, square=True, cmap=\"Blues\", fmt=\".2f\", linewidths=.5)\n",
    "# plt.savefig(\"california_housing_corr_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0cbe7-5894-4e62-846b-b1900a21c6b2",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb60b5f-58e3-4a24-8370-17f671560787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, alpha: float = 1e-7, eps: float = 1e-4) -> None:\n",
    "        self.alpha = alpha  # Learning rate for gradient descent\n",
    "        self.eps = eps  # Threshold of convergence\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"LinearRegression\":\n",
    "        \"\"\"Train the model. Optimization method is gradient descent.\n",
    "\n",
    "        :param X: The feature values of the training data.\n",
    "        :param y: The target values of the training data.\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "        self._m = X.shape[0]  # The number of samples\n",
    "        num_features = X.shape[1]  # The number of features\n",
    "\n",
    "        self._theta = np.zeros(num_features)  # Parameters (weight) of the model (without bias)\n",
    "        self._theta0 = np.zeros(1)  # Bias of the model\n",
    "\n",
    "        self._error_values = []  # The output values of the cost function in each iteration\n",
    "        self._grad_values = []  # Gradient values in each iteration\n",
    "        self._iter_counter = 0  # The counter of iterations\n",
    "        \n",
    "        error = self.J(X, y)  # The initial output value of the cost function with random parameters\n",
    "        diff = 1.0  # The difference between the previous and the current output values of the cost function\n",
    "\n",
    "        # Repeat until convergence\n",
    "        while diff > self.eps:\n",
    "            # Update the parameters by gradient descent\n",
    "            y_pred = self.predict(X)  # Predict the target values with the current parameters\n",
    "            grad = (1 / self._m) * np.dot(y_pred - y, X)  # Calculate the gradient using the formula\n",
    "            self._theta -= self.alpha * grad  # Update the parameters\n",
    "            self._theta0 -= (1 / self._m) * np.sum(y_pred - y)  # Update the bias\n",
    "\n",
    "            # Print the current status\n",
    "            _error = self.J(X, y)  # Compute the error with the updated parameters\n",
    "            diff = abs(error - _error)  # Compute the difference between the previous and the current error\n",
    "            error = _error  # Update the error\n",
    "            self._error_values.append(error)\n",
    "            self._grad_values.append(grad.sum())\n",
    "            self._iter_counter += 1\n",
    "            print(f\"[{self._iter_counter}] error: {error}, diff: {diff}, grad: {grad.sum()}\")\n",
    "        print(f\"Convergence in {self._iter_counter} iterations.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predict the target values using the hypothesis function.\n",
    "\n",
    "        :param X: The feature values of the data.\n",
    "        :return: The predicted target values.\n",
    "        \"\"\"\n",
    "        # Pass the bias and the parameters to the hypothesis function\n",
    "        theta = np.concatenate([self._theta0, self._theta])\n",
    "        return self.h(X, theta)\n",
    "\n",
    "    def h(self, X: pd.DataFrame, theta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Hypothesis function.\n",
    "\n",
    "        :param X: The feature values of the data.\n",
    "        :param theta: The parameters (weight) of the model.\n",
    "        :return: The predicted target values.\n",
    "        \"\"\"\n",
    "        # theta[0] is bias and theta[1:] is parameters\n",
    "        return np.dot(X, theta[1:].T) + theta[0]\n",
    "\n",
    "    def J(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Cost function. Mean squared error (MSE).\n",
    "\n",
    "        :param X: The feature values of the data.\n",
    "        :param y: The target values of the data.\n",
    "        :return: The error value.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)  # Predict the target values with the current parameters\n",
    "        return (1 / (2 * self._m)) * np.sum((y_pred - y) ** 2)  # Compute the error using the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6b10a-84dd-4207-afc1-d9fa7c767e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb8639-667d-4436-8cb3-36ab76099311",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"MSE\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._error_values, color=\"b\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Gradient\")\n",
    "ax.set_ylabel(\"Gradient\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.plot(np.arange(model._iter_counter), model._grad_values, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867eb81-ad1f-4111-848c-154a18f50476",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f7119-615a-47f2-a10c-7540deb29d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(f\"MSE for train data: {mean_squared_error(y_train, y_train_pred)}\")\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(f\"MSE for test data: {mean_squared_error(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dff38c-d10f-4a3f-b901-870a2f22786e",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d9062-417f-4e0f-9465-48fb66497f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # L2 norm\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(f\"MSE for train data: {mean_squared_error(y_train, ridge.predict(X_train))}\")\n",
    "print(f\"MSE for test data: {mean_squared_error(y_test, ridge.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b67a85-b100-44bf-89b3-0f3d04fcac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=1.0)  # L1 norm\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(f\"MSE for train data: {mean_squared_error(y_train, lasso.predict(X_train))}\")\n",
    "print(f\"MSE for test data: {mean_squared_error(y_test, lasso.predict(X_test))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
