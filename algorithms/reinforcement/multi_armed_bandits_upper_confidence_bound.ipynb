{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOcXc8D4tYz+Wk4+OnIo09S",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-10T17:04:22.030156Z",
     "end_time": "2024-02-10T17:04:22.035358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m pip install numpy==1.23.5 matplotlib==3.7.4 tqdm==4.62.3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-10T17:04:27.789949Z",
     "end_time": "2024-02-10T17:04:29.499817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MABEnv:\n",
    "\n",
    "    def __init__(self, n_arms=10, mean=0.0):\n",
    "        \"\"\"Initialize Multi-armed Bandits (MAB) Environment class.\n",
    "\n",
    "        Args:\n",
    "            n_arms: Number of arms to pull.\n",
    "            mean: Mean value for normal distributions.\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.mean = mean\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all the information.\"\"\"\n",
    "        # True reward drawn from the normal distribution with mean for each action\n",
    "        self.q_true = np.random.randn(self.n_arms) + self.mean\n",
    "\n",
    "        # Action index with best true reward\n",
    "        self.optimal_action = np.argmax(self.q_true)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take action and return reward.\n",
    "\n",
    "        Args:\n",
    "            action: Index of next action\n",
    "\n",
    "        Returns:\n",
    "            reward\n",
    "        \"\"\"\n",
    "        # Stochastic reward drawn from the normal distribution\n",
    "        return np.random.randn() + self.q_true[action]\n",
    "\n",
    "    def reward_distribution_example():\n",
    "        plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10))\n",
    "        plt.xlabel(\"Action\")\n",
    "        plt.ylabel(\"Reward distribution\")\n",
    "        plt.savefig('./reward_distribution_example.png')\n",
    "        plt.close()"
   ],
   "metadata": {
    "id": "ksi-3p0SWRWE",
    "ExecuteTime": {
     "start_time": "2024-02-10T17:04:56.816656Z",
     "end_time": "2024-02-10T17:04:57.390970Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EpsilonGreedyAgent:\n",
    "\n",
    "    def __init__(self, env, alpha=None, initial=0.5, epsilon=0.1):\n",
    "        \"\"\"Initialize ε-greedy Agent class.\n",
    "\n",
    "        Args:\n",
    "            env: Multi-armed Bandits (MAB) Environment class.\n",
    "            alpha: Step size parameter to value the latest estimation\n",
    "            initial: Optimistic initial values\n",
    "            epsilon: Probability for exploration.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.initial = initial\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all the information.\"\"\"\n",
    "        # Estimated reward for each action\n",
    "        self.q_estimation = np.zeros(self.env.n_arms) + self.initial\n",
    "\n",
    "        # Chosen times for each action\n",
    "        self.action_count = np.zeros(self.env.n_arms)\n",
    "\n",
    "    def policy(self):\n",
    "        \"\"\"Get index of next action.\n",
    "\n",
    "        Returns:\n",
    "            Index of next action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(np.arange(self.env.n_arms))\n",
    "        return np.argmax(self.q_estimation)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimation.\n",
    "\n",
    "        Args:\n",
    "            action: Index of next action\n",
    "            reward: Reward from environment\n",
    "        \"\"\"\n",
    "        if self.alpha is None:\n",
    "            # Q(A)←Q(A)+1/N(A)[R−Q(A)]\n",
    "            self.action_count[action] += 1\n",
    "            self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])\n",
    "        else:\n",
    "            # Qn+1=Qn+α[Rn−Qn]\n",
    "            self.q_estimation[action] += self.alpha * (reward - self.q_estimation[action])"
   ],
   "metadata": {
    "id": "qi84CoDUY8l7",
    "ExecuteTime": {
     "start_time": "2024-02-10T17:04:57.871943Z",
     "end_time": "2024-02-10T17:04:57.876348Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$A_t \\dot{=} \\underset {a}{\\operatorname {argmax}}\\Biggl[Q_t(a) + c\\sqrt{\\frac{\\log t}{N_t(a)} } \\Biggr]$$"
   ],
   "metadata": {
    "id": "0wI21-Q4U0n_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class UCBAgent:\n",
    "\n",
    "    def __init__(self, env, alpha=None, c=2):\n",
    "        \"\"\"Initialize ε-greedy Agent class.\n",
    "\n",
    "        Args:\n",
    "            env: Multi-armed Bandits (MAB) Environment class.\n",
    "            alpha: Step size parameter to value the latest estimation\n",
    "            c: UCB parameter to control the degree of exploration\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.c = c\n",
    "        self.time = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all the information.\"\"\"\n",
    "        # Estimated reward for each action\n",
    "        self.q_estimation = np.zeros(self.env.n_arms)\n",
    "\n",
    "        # Chosen times for each action\n",
    "        self.action_count = np.zeros(self.env.n_arms)\n",
    "\n",
    "    def policy(self):\n",
    "        \"\"\"Get index of next action.\n",
    "\n",
    "        Returns:\n",
    "            Index of next action\n",
    "        \"\"\"\n",
    "        self.time += 1\n",
    "\n",
    "        ucb_estimation = self.q_estimation + self.c * np.sqrt(np.log(self.time) / (self.action_count + 1e-5))\n",
    "        return np.argmax(ucb_estimation)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimation.\n",
    "\n",
    "        Args:\n",
    "            action: Index of next action\n",
    "            reward: Reward from environment\n",
    "        \"\"\"\n",
    "        if self.alpha is None:\n",
    "            # Q(A)←Q(A)+1/N(A)[R−Q(A)]\n",
    "            self.action_count[action] += 1\n",
    "            self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])\n",
    "        else:\n",
    "            # Qn+1=Qn+α[Rn−Qn]\n",
    "            self.q_estimation[action] += self.alpha * (reward - self.q_estimation[action])"
   ],
   "metadata": {
    "id": "P9WIcIe2WS_r",
    "ExecuteTime": {
     "start_time": "2024-02-10T17:05:00.581939Z",
     "end_time": "2024-02-10T17:05:00.587895Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Simulator:\n",
    "\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "\n",
    "    def run(self, labels, runs=2000, steps=3000):\n",
    "        optimal_action_counts, rewards = self._run(self.agents, runs, steps)\n",
    "\n",
    "        plt.figure(figsize=(10, 20))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for label, rewards in zip(labels, rewards):\n",
    "            plt.plot(rewards, label=label)\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('average reward')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        for label, counts in zip(labels, optimal_action_counts):\n",
    "            plt.plot(counts, label=label)\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('% optimal action')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig('./simulation_result.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _run(self, agents, runs, steps):\n",
    "        optimal_action_counts = np.zeros((len(agents), runs, steps))\n",
    "        rewards = np.zeros(optimal_action_counts.shape)\n",
    "\n",
    "        for i, agent in enumerate(agents):\n",
    "            for r in tqdm(range(runs)):\n",
    "                agent.env.reset()\n",
    "                agent.reset()\n",
    "\n",
    "                for s in range(steps):\n",
    "                    action = agent.policy()\n",
    "                    reward = agent.env.step(action=action)\n",
    "                    agent.update(action=action, reward=reward)\n",
    "\n",
    "                    rewards[i, r, s] = reward\n",
    "                    if action == agent.env.optimal_action:\n",
    "                        optimal_action_counts[i, r, s] = 1\n",
    "\n",
    "        optimal_action_counts = optimal_action_counts.mean(axis=1)\n",
    "        rewards = rewards.mean(axis=1)\n",
    "\n",
    "        return optimal_action_counts, rewards"
   ],
   "metadata": {
    "id": "A-OM8VPbWU3n",
    "ExecuteTime": {
     "start_time": "2024-02-10T17:05:01.867442Z",
     "end_time": "2024-02-10T17:05:01.871771Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = MABEnv(n_arms=10, mean=0.0)\n",
    "epsilons = [0, 0.1, 0.01, 0.5, 1.0]\n",
    "agents = [EpsilonGreedyAgent(env=env, epsilon=epsilon) for epsilon in epsilons]\n",
    "labels=[f\"EpsilonGreedyAgent epsilon={epsilon}\" for epsilon in epsilons]\n",
    "c_params = [0, 1, 2]\n",
    "agents += [UCBAgent(env=env, c=c) for c in c_params]\n",
    "labels += [f\"UCBAgent c={c}\" for c in c_params]\n",
    "simulator = Simulator(agents=agents)\n",
    "simulator.run(labels=labels, runs=2000, steps=3000)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1x4Tp6AWV70",
    "outputId": "462c8277-83bd-4a5f-dc48-9437d894702f"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
