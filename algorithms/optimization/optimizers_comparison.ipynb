{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing Libraries (Python version >= 3.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "version = sys.version_info\n",
    "print(version)\n",
    "assert version.major == 3 and version.minor >= 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.088591Z",
     "end_time": "2024-02-11T14:17:47.149647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m pip install numpy == 1.23.5 matplotlib == 3.7.4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.094339Z",
     "end_time": "2024-02-11T14:17:47.234576Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "W_{t+1} = W_t - \\eta\\frac{\\partial L}{\\partial W_t} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_t: Parameters\\,(Weights) \\\\\n",
    "\\eta: Learning\\,Rate \\\\\n",
    "L: Loss\\,Function \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        \"\"\"Initialize instance.\n",
    "\n",
    "        :param lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params: dict, grads: dict) -> None:\n",
    "        \"\"\"Update parameters.\n",
    "\n",
    "        :param params: Parameters.\n",
    "        :param grads: Gradients.\n",
    "        \"\"\"\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.229301Z",
     "end_time": "2024-02-11T14:17:47.239712Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "v_{t+1} = \\alpha v_t - \\eta\\frac{\\partial L}{\\partial W_t} \\\\\n",
    "W_{t+1} = W_t + v_{t+1} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha: Momentum\\,Coefficient \\\\\n",
    "v_t: Velocity \\\\\n",
    "\\eta: Learning\\,Rate \\\\\n",
    "L: Loss\\,Function \\\\\n",
    "W_t: Parameters\\,(Weights) \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    \"\"\"Momentum.\"\"\"\n",
    "\n",
    "    def __init__(self, lr: float = 0.01, momentum: float = 0.9) -> None:\n",
    "        \"\"\"Initialize instance.\n",
    "\n",
    "        :param lr: Learning rate.\n",
    "        :param momentum: Momentum.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None  # velocity\n",
    "\n",
    "    def update(self, params: dict, grads: dict) -> None:\n",
    "        \"\"\"Update parameters.\n",
    "\n",
    "        :param params: Parameters.\n",
    "        :param grads: Gradients.\n",
    "        \"\"\"\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.235702Z",
     "end_time": "2024-02-11T14:17:47.239917Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "h_{t+1} = h_t + \\frac{\\partial L}{\\partial W_t} \\odot \\frac{\\partial L}{\\partial W_t} \\\\\n",
    "W_{t+1} = W_t - \\eta\\frac{1}{\\sqrt{h_{t+1}} + \\epsilon} \\odot \\frac{\\partial L}{\\partial W_t} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t: Squared\\,Gradients \\\\\n",
    "\\eta: Learning\\,Rate \\\\\n",
    "\\epsilon: Smoothing\\,Term \\\\\n",
    "L: Loss\\,Function \\\\\n",
    "W_t: Parameters\\,(Weights) \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"AdaGrad.\"\"\"\n",
    "\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        \"\"\"Initialize instance.\n",
    "\n",
    "        :param lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.h = None  # squared gradients\n",
    "\n",
    "    def update(self, params: dict, grads: dict) -> None:\n",
    "        \"\"\"Update parameters.\n",
    "\n",
    "        :param params: Parameters.\n",
    "        :param grads: Gradients.\n",
    "        \"\"\"\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.243745Z",
     "end_time": "2024-02-11T14:17:47.245246Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "m_{t+1} = \\beta_1 m_t + (1 - \\beta_1)\\frac{\\partial L}{\\partial W_t} \\\\\n",
    "v_{t+1} = \\beta_2 v_t + (1 - \\beta_2)\\frac{\\partial L}{\\partial W_t} \\odot \\frac{\\partial L}{\\partial W_t} \\\\\n",
    "\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^t} \\\\\n",
    "\\hat{v}_{t+1} = \\frac{v_{t+1}}{1 - \\beta_2^t} \\\\\n",
    "W_{t+1} = W_t - \\eta\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_1: Exponential\\,Decay\\,Rate\\,for\\,the\\,First\\,Moment\\,Estimate \\\\\n",
    "\\beta_2: Exponential\\,Decay\\,Rate\\,for\\,the\\,Second\\,Moment\\,Estimate \\\\\n",
    "m_t: First\\,Moment\\,Estimate \\\\\n",
    "v_t: Second\\,Moment\\,Estimate \\\\\n",
    "\\eta: Learning\\,Rate \\\\\n",
    "\\epsilon: Smoothing\\,Term \\\\\n",
    "L: Loss\\,Function \\\\\n",
    "W_t: Parameters\\,(Weights) \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Adam.\"\"\"\n",
    "\n",
    "    def __init__(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999) -> None:\n",
    "        \"\"\"Initialize instance.\n",
    "\n",
    "        :param lr: Learning rate.\n",
    "        :param beta1: Exponential decay rate for the first moment estimates.\n",
    "        :param beta2: Exponential decay rate for the second moment estimates.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0  # iteration\n",
    "        self.m = None  # first moment estimates\n",
    "        self.v = None  # second moment estimates\n",
    "\n",
    "    def update(self, params: dict, grads: dict) -> None:\n",
    "        \"\"\"Update parameters.\n",
    "\n",
    "        :param params: Parameters.\n",
    "        :param grads: Gradients.\n",
    "        \"\"\"\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])\n",
    "\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.266838Z",
     "end_time": "2024-02-11T14:17:47.303652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return x ** 2 / 20.0 + y ** 2\n",
    "\n",
    "\n",
    "def df(x, y):\n",
    "    return x / 10.0, 2.0 * y\n",
    "\n",
    "\n",
    "init_pos = (-7.0, 2.0)\n",
    "params = {\n",
    "    \"x\": init_pos[0],\n",
    "    \"y\": init_pos[1],\n",
    "}\n",
    "grads = {\n",
    "    \"x\": 0,\n",
    "    \"y\": 0,\n",
    "}\n",
    "optimizers = {\n",
    "    \"SGD\": SGD(lr=0.95),\n",
    "    \"Momentum\": Momentum(lr=0.1),\n",
    "    \"AdaGrad\": AdaGrad(lr=1.5),\n",
    "    \"Adam\": Adam(lr=0.3),\n",
    "}\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for key in optimizers:\n",
    "    optimizer = optimizers[key]\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    params[\"x\"], params[\"y\"] = init_pos[0], init_pos[1]\n",
    "\n",
    "    for i in range(30):\n",
    "        x_history.append(params[\"x\"])\n",
    "        y_history.append(params[\"y\"])\n",
    "\n",
    "        grads[\"x\"], grads[\"y\"] = df(params[\"x\"], params[\"y\"])\n",
    "        optimizer.update(params, grads)\n",
    "\n",
    "    x = np.arange(-10, 10, 0.01)\n",
    "    y = np.arange(-5, 5, 0.01)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # for simple contour line\n",
    "    mask = Z > 7\n",
    "    Z[mask] = 0\n",
    "\n",
    "    # plot\n",
    "    plt.subplot(2, 2, idx)\n",
    "    idx += 1\n",
    "    plt.plot(x_history, y_history, \"o-\", color=\"red\")\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.plot(0, 0, \"+\")\n",
    "    plt.title(key)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T14:17:47.272318Z",
     "end_time": "2024-02-11T14:17:48.958201Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
